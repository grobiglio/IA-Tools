[Inicio](./README.md) | [Anterior (2. Markdown)](./1-Markdown.md) | [Pr√≥ximo (4. Ingenier√≠a de Prompts)](./3-IngPrompts.md)

---

# Fundamentos de ChatGPT y conceptos clave

**Objetivo**: Comprender c√≥mo funciona ChatGPT, su estructura y los conceptos esenciales para interactuar eficazmente.

## Introducci√≥n a ChatGPT

### ¬øQu√© es ChatGPT?

[ChatGPT](https://es.wikipedia.org/wiki/ChatGPT) (Chat Generative Pre-Trained Transformer) es un sistema conversacional basado en inteligencia artificial desarrollado por OpenAI. Utiliza modelos avanzados de lenguaje natural llamados **GPT (Generative Pre-trained Transformer)** para comprender, procesar y generar texto en m√∫ltiples idiomas.

ChatGPT es capaz de:
- responder preguntas,
- redactar textos de todo tipo,
- generar c√≥digo en diversos lenguajes de programaci√≥n,
- analizar documentos e im√°genes,
- traducir y resumir informaci√≥n,
- asistir en tareas acad√©micas y profesionales, entre muchas otras funciones.

Desde 2025, la versi√≥n disponible en el plan **ChatGPT Plus** utiliza **GPT-5**, un modelo multimodal (texto, im√°genes y audio) m√°s preciso, veloz y con mejor memoria contextual que su predecesor GPT-4-turbo. Adem√°s, incluye acceso a herramientas como **b√∫squeda web integrada, visor de archivos, Python para an√°lisis de datos, proyectos colaborativos, tareas programadas y memoria persistente.**

---

## Breve historia de los modelos de lenguaje

El desarrollo de ChatGPT se inscribe en la evoluci√≥n del **Procesamiento de Lenguaje Natural (PLN)**, un √°rea de la inteligencia artificial que busca ense√±ar a las m√°quinas a comprender, interpretar y generar lenguaje humano.

### De las reglas al aprendizaje profundo
- **D√©cadas de 1960-1980:** Los primeros sistemas (como ELIZA y SHRDLU) se basaban en reglas gramaticales simples y bases de datos cerradas.
- **D√©cadas de 1990-2000:** Se introdujeron modelos estad√≠sticos como los n-gramas y algoritmos de aprendizaje autom√°tico como Naive Bayes y SVM.
- **A√±os 2010:** Aparecen modelos de **aprendizaje profundo**, como Word2Vec y las redes neuronales recurrentes (RNN), capaces de manejar relaciones m√°s complejas entre palabras.

### La revoluci√≥n de los Transformers
- En 2017, Google presenta el paper *‚ÄúAttention is All You Need‚Äù*, que introduce la arquitectura **Transformer**. Esta innovaci√≥n permite entrenar modelos m√°s grandes, con mejor comprensi√≥n contextual.
- En 2018-2020 surgen modelos masivos como **BERT (Google)**, **GPT-2 (OpenAI)** y **T5**, que comienzan a usarse en tareas del mundo real.

### GPT y los grandes modelos actuales
- **GPT-3 (2020):** 175 mil millones de par√°metros, sorprendente en redacci√≥n, programaci√≥n y razonamiento.
- **GPT-4 (2023):** Modelo multimodal (texto e im√°genes), m√°s preciso y seguro.
- **GPT-4-turbo (2023):** Versi√≥n optimizada, m√°s r√°pida y econ√≥mica.
- **GPT-5 (2025):** Modelo multimodal avanzado con integraci√≥n de texto, im√°genes y audio, mayor coherencia en di√°logos largos y mejor memoria persistente.

En paralelo, otras empresas desarrollaron modelos competitivos:
- **Claude 3 (Anthropic, 2024):** Enfoque en seguridad y control de la IA.
- **Gemini 1.5 (Google DeepMind, 2024):** Multimodal con capacidades extendidas en razonamiento.
- **LLaMA 3 (Meta, 2024):** Open source, ampliamente adoptado en investigaci√≥n.
- **Mistral / Mixtral (2024):** Modelos abiertos con arquitecturas eficientes.
- **Grok 2 (xAI, 2025):** Integrado a la plataforma X (ex Twitter), con acceso a datos en tiempo real.

### Impacto y futuro
Hoy en d√≠a, estos modelos se aplican en educaci√≥n, salud, ingenier√≠a, derecho, atenci√≥n al cliente y creatividad digital, transformando la forma en que interactuamos con la tecnolog√≠a.  
A la vez, plantean desaf√≠os √©ticos: sesgo en los datos, privacidad y riesgo de uso indebido. El futuro apunta a una integraci√≥n a√∫n mayor en la vida diaria y profesional.

---

## ¬øC√≥mo funciona ChatGPT?

ChatGPT se basa en un modelo de lenguaje entrenado para predecir texto de forma contextual. Su arquitectura **Transformer** utiliza un mecanismo de **atenci√≥n** que analiza relaciones entre palabras, frases e ideas.

### Arquitectura basada en Transformers
A diferencia de las redes tradicionales, los Transformers permiten procesar m√∫ltiples palabras en paralelo y captar relaciones complejas a larga distancia, lo que mejora la coherencia de las respuestas.

- El mecanismo de **atenci√≥n** asigna relevancia a cada palabra dentro del contexto.
- El modelo no ‚Äúlee‚Äù de manera secuencial, sino que comprende **relaciones globales**.

### Generaci√≥n de texto paso a paso

1. **Tokenizaci√≥n**  
   El texto se divide en *tokens*.  
   Ejemplo:  
   - "inteligencia artificial" ‚Üí `["int", "eligencia", " artificial"]`

2. **Predicci√≥n probabil√≠stica**  
   El modelo calcula cu√°l es el siguiente token m√°s probable considerando el contexto.

3. **Selecci√≥n del siguiente token**  
   Puede elegir la opci√≥n m√°s probable o aplicar muestreo para diversificar resultados.

Este proceso se repite hasta formar frases completas.

### Contexto y memoria
ChatGPT considera todo el historial de la conversaci√≥n dentro de una **ventana de contexto** de hasta **128.000 tokens** en GPT-4-turbo y GPT-5.  
Adem√°s:
- Mantiene coherencia a lo largo de di√°logos largos.  
- Con la **memoria persistente** (2024 en adelante), puede recordar informaci√≥n entre sesiones, como tu nombre o preferencias de estilo.  

### Instrucciones del sistema y roles
Al iniciar una conversaci√≥n, ChatGPT recibe una **instrucci√≥n oculta del sistema** (system prompt) que define su comportamiento.  
El usuario tambi√©n puede personalizar estas instrucciones para ajustar el tono y estilo de las respuestas.

### Capacidad de adaptaci√≥n
ChatGPT adapta su estilo al del usuario:
- Puede ser t√©cnico, creativo, formal o coloquial.  
- Responde en el idioma y con el nivel de detalle solicitado.  

---

## Concepto de tokens

Los modelos de lenguaje procesan el texto en peque√±as unidades llamadas **tokens**.

### ¬øQu√© es un token?
Un *token* puede ser:
- Una palabra (`ma√±ana`)
- Una parte de palabra (`incre`, `√≠ble`)
- Un s√≠mbolo (`?`, `¬°`)
- Un espacio

Ejemplo:  
‚ÄúHola, ¬øc√≥mo est√°s?‚Äù ‚Üí `["Hola", ",", "¬ø", "c√≥mo", "est√°s", "?"]`

> Regla pr√°ctica: **1 token ‚âà 4 caracteres en ingl√©s** (en espa√±ol suele ser un poco m√°s).

### ¬øPara qu√© sirven los tokens?
- El modelo no analiza frases completas, sino secuencias de tokens.  
- Calcula probabilidades de cu√°l ser√° el siguiente token.  
- Tanto la entrada del usuario como la salida del modelo consumen tokens.

### L√≠mite de tokens por conversaci√≥n
Cada modelo tiene una **ventana de contexto**:

| Modelo              | L√≠mite de tokens |
|---------------------|------------------|
| GPT-3.5             | 16.000           |
| GPT-4-turbo         | 128.000          |
| GPT-5               | 128.000          |

Cuando se alcanza este l√≠mite:
- ChatGPT comienza a olvidar mensajes anteriores.  
- O bien la sesi√≥n se reinicia.  

üëâ Herramienta √∫til: [Tokenizador de OpenAI](https://platform.openai.com/tokenizer)

---

## Limitaciones y buenas pr√°cticas

### Limitaciones del modelo
- **Alucinaciones:** puede inventar informaci√≥n.  
- **Sesgos:** refleja prejuicios de los datos de entrenamiento.  
- **Dependencia de contexto:** si el prompt es vago, la respuesta puede ser poco √∫til.  
- **Conexi√≥n a internet limitada:** la b√∫squeda integrada mejora esto, pero no siempre garantiza fuentes confiables.  

### Buenas pr√°cticas para interactuar
- **Validar la informaci√≥n t√©cnica** con fuentes oficiales.  
- **Dar contexto claro** en los prompts.  
- **Dividir textos largos** en partes manejables.  
- **Combinar ChatGPT con otras herramientas** para obtener resultados m√°s robustos.  

---

[Inicio](./README.md) | [Anterior (2. Markdown)](./1-Markdown.md) | [Pr√≥ximo (4. Ingenier√≠a de Prompts)](./3-IngPrompts.md)