# Introducci√≥n General a ChatGPT: Conceptos Fundamentales y Uso Eficiente

Basado en el curso de Udemy [ChatGPT Complete Guide: Learn Generative AI, ChatGPT & More](https://www.udemy.com/course/complete-ai-guide/?couponCode=KEEPLEARNING)

## Objetivo
Familiarizar a los participantes con el funcionamiento b√°sico de ChatGPT, incluyendo conceptos clave como ingenier√≠a de prompts, tokens y las herramientas integradas para optimizar su uso.

## Contenido

- Markdown
- Fundamentos de ChatGPT y conceptos clave

### Markdown

**Objetivo:** Introducir a los participantes en el uso b√°sico de Markdown para estructurar y formatear texto de manera eficiente.

#### ¬øQu√© es Markdown?

[Markdown](https://es.wikipedia.org/wiki/Markdown) es un lenguaje de marcado ligero que permite formatear texto utilizando una sintaxis simple.

**Ventajas sobre otros editores:** es f√°cil de leer y escribir, y el resultado es limpio y bien estructurado.

**Utilidad de Markdown**  
- Creaci√≥n de documentaci√≥n t√©cnica.  
- Estructuraci√≥n de reportes y presentaciones.  
- Integraci√≥n con plataformas como GitHub, Bitbucket, etc.
- Es el lenguaje que usa ChatGPT.

#### Sintaxis b√°sica de Markdown

**T√≠tulos y encabezados:**

- Uso de `#` para encabezados de diferentes niveles.  
- Ejemplo:
    ```markdown
    # T√≠tulo de nivel 1
    ## T√≠tulo de nivel 2
    ### T√≠tulo de nivel 3
    ```

**Listas:**  
- Listas numeradas y no numeradas.  
- Ejemplo:
    ```markdown
    - Elemento de lista
    - Otro elemento de lista
    1. Elemento numerado
    2. Otro elemento numerado
    ```

**Negrita, cursiva y enlaces:**
- C√≥mo resaltar texto, crear enlaces y agregar im√°genes.  
- Ejemplo:
    ```markdown
    **Texto en negrita**
    *Texto en cursiva*
    [Enlace a un sitio web](http://example.com)
    ![Texto alternativo de la imagen](imagen.jpg)
    ```

**Bloques de c√≥digo:**
- Formateo de c√≥digo con comillas invertidas (backticks).  
- Ejemplo:
    ```markdown
    `C√≥digo en l√≠nea`
    ```
    ```markdown
    ```python
    print("Hola, mundo")
    ```
    ```

**Tablas:**
- C√≥mo crear tablas para organizar informaci√≥n.
- Ejemplo:
    ```markdown
    | Columna 1 | Columna 2 | Columna 3 |
    |-----------|-----------|-----------|
    | Fila 1    | Dato 1    | Dato 2    |
    | Fila 2    | Dato 3    | Dato 4    |
    ```

**Citas y comentarios:**
- C√≥mo agregar citas y comentarios para mayor claridad.
- Ejemplo:
    ```markdown
    > Esto es una cita.
    <!-- Esto es un comentario -->
    ```

**Listas de tareas:**
- Creaci√≥n de listas de tareas con casillas de verificaci√≥n.  
- Ejemplo:
    ```markdown
    - [ ] Tarea 1
    - [x] Tarea completada
    ```

En los siguientes enlaces hay m√°s y muy completa informaci√≥n para aprender Markdown:
- [The Ultimate Markdown Guide](https://medium.com/analytics-vidhya/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fd)
- [Basic writing and formatting syntax](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax)
- [Markdown Guide](https://www.markdownguide.org/)
- [Markdown Preview](https://www.digitalocean.com/community/markdown)

#### Editores y plataformas

- [Visual Studio Code](https://code.visualstudio.com/) u otro editor de texto compatible con Markdown.  
- [GitHub](https://github.com/) u otras plataformas.

### Fundamentos de ChatGPT y conceptos clave

**Objetivo**: Comprender c√≥mo funciona ChatGPT, su estructura y los conceptos esenciales para interactuar eficazmente.

#### Introducci√≥n a ChatGP

##### **¬øQu√© es ChatGPT?**

[ChatGPT](https://es.wikipedia.org/wiki/ChatGPT) (Chat Generative Pre-Trained Transformer) es un modelo avanzado de lenguaje desarrollado por OpenAI que utiliza inteligencia artificial para comprender y generar texto de manera coherente. Dise√±ado para asistir en tareas como responder preguntas, generar contenido, traducir, programar y m√°s, ChatGPT interact√∫a de forma conversacional, adapt√°ndose a diversos contextos y necesidades.

###### Historia breve de los modelos de lenguaje.

El desarrollo de los [modelos de lenguaje](https://es.wikipedia.org/wiki/Modelaci%C3%B3n_del_lenguaje) est√° √≠ntimamente ligado al avance del **Procesamiento de Lenguaje Natural (PLN)**, una rama de la Inteligencia Artificial (IA) que busca permitir que las m√°quinas comprendan, interpreten y generen lenguaje humano de manera efectiva. Desde sus inicios, el [PLN](https://es.wikipedia.org/wiki/Procesamiento_de_lenguajes_naturales) ha evolucionado desde t√©cnicas estad√≠sticas b√°sicas hasta los impresionantes Grandes Modelos de Lenguaje ([LLM](https://es.wikipedia.org/wiki/Modelo_extenso_de_lenguaje)) que conocemos hoy en d√≠a.

**Los primeros modelos de lenguaje**
A mediados del siglo XX, con la llegada de la inform√°tica, el inter√©s por ense√±ar a las m√°quinas a procesar lenguaje humano tom√≥ forma. En esta etapa inicial, se desarrollaron enfoques basados en reglas, donde los sistemas utilizaban gram√°ticas y diccionarios predefinidos. Ejemplos de estos sistemas incluyen ELIZA (1966), que simulaba una conversaci√≥n terap√©utica, y SHRDLU (1972), dise√±ado para entender y responder preguntas en un entorno limitado.

Con el tiempo, las t√©cnicas estad√≠sticas comenzaron a reemplazar los enfoques basados en reglas. Durante la d√©cada de 1980 y 1990, los modelos n-gram, que predec√≠an palabras bas√°ndose en su probabilidad en relaci√≥n con las palabras anteriores, se volvieron populares. Aunque simples, estos modelos establecieron las bases para trabajos m√°s avanzados.

**Procesamiento de Lenguaje Natural y aprendizaje autom√°tico**
En el cambio de milenio, el PLN comenz√≥ a incorporar algoritmos de **aprendizaje autom√°tico**. M√©todos como las M√°quinas de Soporte Vectorial (SVM) y los clasificadores bayesianos fueron empleados para tareas como an√°lisis de sentimiento y categorizaci√≥n de texto. Sin embargo, estos enfoques requer√≠an caracter√≠sticas cuidadosamente dise√±adas a mano, lo que limitaba su escalabilidad y aplicabilidad.

El verdadero cambio ocurri√≥ con el auge del **aprendizaje profundo** en la d√©cada de 2010. Los modelos de redes neuronales, como Word2Vec (2013), introdujeron representaciones vectoriales de palabras (embeddings), lo que permiti√≥ capturar relaciones sem√°nticas entre palabras. Poco despu√©s, las redes recurrentes ([RNN](https://es.wikipedia.org/wiki/Redes_neuronales_recurrentes)) y sus variantes como LSTM y GRU mejoraron la capacidad de los modelos para manejar secuencias de texto m√°s largas.

**La era de los Grandes Modelos de Lenguaje (LLM)**
El avance decisivo lleg√≥ con la introducci√≥n de los modelos basados en **Transformers**, como el modelo Transformer original (2017). Este dise√±o, que utiliza mecanismos de atenci√≥n para procesar datos en paralelo, revolucion√≥ el PLN al superar las limitaciones de las redes recurrentes. En 2018, OpenAI present√≥ GPT (Generative Pre-trained Transformer), seguido por modelos m√°s avanzados como GPT-2, GPT-3 y GPT-4. Paralelamente, Google desarroll√≥ BERT, dise√±ado para comprender el contexto bidireccional en el texto.

Los **LLM** modernos como GPT-4 cuentan con miles de millones de par√°metros y son preentrenados en enormes corpus de datos textuales. Estos modelos han demostrado capacidades sorprendentes en traducci√≥n, generaci√≥n de texto, programaci√≥n y razonamiento, marcando un hito en el desarrollo del lenguaje computacional.

**Impacto y futuro**
Hoy en d√≠a, los modelos de lenguaje son esenciales en aplicaciones como asistentes virtuales, an√°lisis de datos y educaci√≥n. Sin embargo, tambi√©n plantean desaf√≠os √©ticos, como el sesgo en los datos y el uso indebido. A medida que avanza la IA, se espera que los LLM se integren a√∫n m√°s profundamente en nuestras vidas, cambiando c√≥mo interactuamos con la tecnolog√≠a y entre nosotros.

![Un cuadro que muestra el futuro de la IA](./Imagenes/AI-future.jpg)

Para m√°s informaci√≥n sobre el futuro de la IA leer este art√≠culo üëâ [AI Top-of-Mind for 7.16.24 ‚Äî Our AI Future](https://medium.com/a-i-society/ai-top-of-mind-for-7-16-24-our-ai-future-07f1bd9b1e41)

###### C√≥mo se entrenan (procesamiento de texto masivo y aprendizaje supervisado)

El entrenamiento de los **grandes modelos de lenguaje (LLM)**, como GPT, sigue un proceso complejo que combina el procesamiento de texto masivo y t√©cnicas de aprendizaje supervisado y no supervisado.

1. **Procesamiento de texto masivo**
    Los LLM se entrenan en grandes cantidades de datos textuales provenientes de libros, sitios web, art√≠culos, foros y m√°s. Estos textos se limpian para eliminar informaci√≥n irrelevante o inapropiada, como duplicados, errores ortogr√°ficos extremos o contenido que viola est√°ndares √©ticos. Luego, el texto se tokeniza, dividi√©ndose en fragmentos peque√±os (como palabras o subpalabras) que el modelo puede procesar.

2. **Preentrenamiento no supervisado**
    El modelo comienza con un preentrenamiento en el que aprende a predecir la siguiente palabra en una secuencia de texto o a llenar palabras faltantes. Esto utiliza un enorme corpus de texto sin necesidad de etiquetas humanas. T√©cnicas como el **enmascaramiento bidireccional** (en modelos como BERT) o la predicci√≥n de la siguiente palabra (en GPT) permiten al modelo captar relaciones sem√°nticas y patrones ling√º√≠sticos.

3. **Ajuste fino (aprendizaje supervisado)**
    Tras el preentrenamiento, los modelos suelen someterse a un ajuste fino utilizando conjuntos de datos m√°s peque√±os y espec√≠ficos, etiquetados por humanos. Este paso los adapta a tareas concretas, como traducci√≥n, generaci√≥n de texto o soporte t√©cnico.

4. **Retroalimentaci√≥n humana**
    Para mejorar la calidad de las respuestas, los modelos pueden entrenarse usando **aprendizaje por refuerzo a partir de retroalimentaci√≥n humana (RLHF)**, donde evaluadores humanos clasifican las respuestas del modelo, gui√°ndolo hacia respuestas m√°s √∫tiles y precisas.

##### **¬øC√≥mo funciona ChatGPT?**

ChatGPT se basa en los **Transformers**, una arquitectura de redes neuronales que utiliza un mecanismo de atenci√≥n para procesar y generar texto. Su capacidad para interpretar y responder a las entradas de los usuarios depende de su entrenamiento en enormes cantidades de datos textuales y del uso de algoritmos avanzados que priorizan el contexto y las relaciones entre palabras.

###### Generaci√≥n de texto basada en predicciones probabil√≠sticas
El n√∫cleo del funcionamiento de ChatGPT es su capacidad para generar texto previendo cu√°l ser√° la pr√≥xima palabra m√°s probable en funci√≥n del contexto. Durante el entrenamiento:  

1. **Tokenizaci√≥n**: El texto de entrada se convierte en tokens (fracciones de palabras o subpalabras). Por ejemplo, "Hola, ¬øc√≥mo est√°s?" podr√≠a dividirse en "[Hola, ,, ¬ø, c√≥mo, est√°s, ?]".  
   
2. **Asignaci√≥n de probabilidad**: El modelo calcula la probabilidad de cada posible token siguiente en funci√≥n de los tokens anteriores. Por ejemplo, despu√©s de "Hola, ¬øc√≥mo", las palabras "est√°s" o "te va" tienen altas probabilidades, pero palabras como "computadora" tienen probabilidades mucho menores.  

3. **Selecci√≥n y generaci√≥n**: El modelo elige el token m√°s probable (o una combinaci√≥n ponderada para mayor variabilidad) y lo a√±ade a la secuencia generada. Este proceso contin√∫a hasta que se alcanza una longitud deseada o un token de finalizaci√≥n.

###### C√≥mo ChatGPT interpreta las entradas de los usuarios
Cuando un usuario env√≠a un mensaje, ChatGPT procesa el texto de entrada como un todo, capturando no solo las palabras individuales, sino tambi√©n sus relaciones en el contexto. Esto se logra mediante:

- **Mecanismo de atenci√≥n**: Cada palabra de la entrada influye en las dem√°s seg√∫n su relevancia, permitiendo que el modelo entienda dependencias incluso en frases largas o complejas.
- **Contexto acumulativo**: ChatGPT considera no solo el mensaje m√°s reciente, sino tambi√©n el historial de la conversaci√≥n. Esto le permite ofrecer respuestas coherentes y relevantes en interacciones m√°s largas.

###### C√≥mo ChatGPT genera respuestas
Bas√°ndose en el contexto proporcionado, el modelo predice y genera la respuesta token por token.  
- Si el usuario pregunta algo ambiguo, el modelo intenta dar una respuesta general o solicita aclaraciones.  
- Si la entrada es espec√≠fica, busca patrones en su conocimiento preentrenado para proporcionar detalles precisos.  

En resumen, ChatGPT utiliza c√°lculos probabil√≠sticos y el contexto proporcionado para generar texto fluido y relevante. Su dise√±o lo hace capaz de adaptarse a diferentes estilos, interpretar la intenci√≥n detr√°s de las entradas y producir respuestas √∫tiles.

---

## Material de soporte

### Apuntes

Acceder desde üëâ [aqu√≠](https://fadeasa-my.sharepoint.com/:f:/g/personal/robiglio_fadeasa_com_ar/Eria57XL0fFGorIPXN18dnwB244nMNau94q5K65IpPCKwA?e=lAFxyo) a los apuntes.

### Lista de art√≠culos

- [ChatGPT vs. Google](https://leapyearlearning.mykajabi.com/blog/chatgpt-vs-google-key-differences-and-when-to-use-each)
- [Comparing ChatGPT-4o, ChatGPT-4o Mini, and ChatGPT-4](https://leapyearlearning.mykajabi.com/blog/prompts-for-increasing-business-productivity)
- [Prompting](https://leapyearlearning.mykajabi.com/blog/prompting-the-art-of-creating-effective-prompts-for-llm-s)
- [Awesome ChatGPT prompts in GitHub](https://github.com/f/awesome-chatgpt-prompts)
- [What are tokens and how to count them?](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)
- [Tokenizer](https://platform.openai.com/tokenizer)
- [200 Best ChatGPT Prompts for 2024](https://leapyearlearning.mykajabi.com/blog/week-1)
- [A Comprehensive Look Into Prompt Engineering Fundamentals](https://leapyearlearning.mykajabi.com/blog/a-comprehensive-guide-to-prompt-engineering)